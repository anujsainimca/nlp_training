{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIDF Vectorizer\n",
    "TfIDF vectorizer is most famous and most widely used vectoirzer in NLP. TF-IDF(term frequency inverse document frequency) is an extension of TF (Term Frequency) which calculates weights for each token. Here we also give negative weights if a word is occuring too frequent in whole corpus. So for example if words like (in,of the) may have good count in a document still their tf-idf score will be quite less as their document frequency is also quite high.\n",
    "\n",
    "tfidf=count of token t in a single document/count of token t across all documents\n",
    "\n",
    "<i>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if smooth_idf=False), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate TfidfVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df=5)\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Build Classification model using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'000',\n",
       " u'0003',\n",
       " u'01',\n",
       " u'02',\n",
       " u'04',\n",
       " u'05',\n",
       " u'06',\n",
       " u'07',\n",
       " u'0700',\n",
       " u'08',\n",
       " u'09',\n",
       " u'10',\n",
       " u'100',\n",
       " u'106',\n",
       " u'11',\n",
       " u'110',\n",
       " u'112',\n",
       " u'115',\n",
       " u'12',\n",
       " u'120',\n",
       " u'122',\n",
       " u'123',\n",
       " u'125',\n",
       " u'128',\n",
       " u'13',\n",
       " u'130',\n",
       " u'135',\n",
       " u'14',\n",
       " u'15',\n",
       " u'150',\n",
       " u'154',\n",
       " u'156',\n",
       " u'16',\n",
       " u'160',\n",
       " u'17',\n",
       " u'171',\n",
       " u'1736',\n",
       " u'18',\n",
       " u'180',\n",
       " u'1800',\n",
       " u'1812',\n",
       " u'19',\n",
       " u'1949',\n",
       " u'198',\n",
       " u'1980s',\n",
       " u'1987',\n",
       " u'1989',\n",
       " u'1990',\n",
       " u'1990s',\n",
       " u'1991',\n",
       " u'1992',\n",
       " u'1993',\n",
       " u'1994',\n",
       " u'1995',\n",
       " u'1996',\n",
       " u'1997',\n",
       " u'1998',\n",
       " u'1999',\n",
       " u'20',\n",
       " u'200',\n",
       " u'2000',\n",
       " u'202',\n",
       " u'21',\n",
       " u'212',\n",
       " u'215',\n",
       " u'22',\n",
       " u'23',\n",
       " u'230',\n",
       " u'24',\n",
       " u'2423',\n",
       " u'25',\n",
       " u'250',\n",
       " u'26',\n",
       " u'27',\n",
       " u'28',\n",
       " u'284',\n",
       " u'29',\n",
       " u'290',\n",
       " u'30',\n",
       " u'300',\n",
       " u'31',\n",
       " u'312',\n",
       " u'32',\n",
       " u'325',\n",
       " u'33',\n",
       " u'34',\n",
       " u'35',\n",
       " u'350',\n",
       " u'36',\n",
       " u'37',\n",
       " u'370',\n",
       " u'373',\n",
       " u'375',\n",
       " u'38',\n",
       " u'39',\n",
       " u'40',\n",
       " u'400',\n",
       " u'408',\n",
       " u'41',\n",
       " u'42',\n",
       " u'43',\n",
       " u'44',\n",
       " u'45',\n",
       " u'450',\n",
       " u'46',\n",
       " u'47',\n",
       " u'48',\n",
       " u'49',\n",
       " u'50',\n",
       " u'500',\n",
       " u'51',\n",
       " u'52',\n",
       " u'53',\n",
       " u'54',\n",
       " u'542',\n",
       " u'55',\n",
       " u'56',\n",
       " u'57',\n",
       " u'58',\n",
       " u'59',\n",
       " u'5956',\n",
       " u'60',\n",
       " u'600',\n",
       " u'61',\n",
       " u'62',\n",
       " u'625',\n",
       " u'63',\n",
       " u'64',\n",
       " u'65',\n",
       " u'650',\n",
       " u'66',\n",
       " u'67',\n",
       " u'68',\n",
       " u'69',\n",
       " u'70',\n",
       " u'700',\n",
       " u'71',\n",
       " u'72',\n",
       " u'735',\n",
       " u'74',\n",
       " u'75',\n",
       " u'750',\n",
       " u'76',\n",
       " u'77',\n",
       " u'7717',\n",
       " u'78',\n",
       " u'79',\n",
       " u'80',\n",
       " u'800',\n",
       " u'81',\n",
       " u'82',\n",
       " u'83',\n",
       " u'8312',\n",
       " u'84',\n",
       " u'85',\n",
       " u'859',\n",
       " u'86',\n",
       " u'87',\n",
       " u'875',\n",
       " u'8787',\n",
       " u'88',\n",
       " u'898',\n",
       " u'90',\n",
       " u'900',\n",
       " u'91',\n",
       " u'92',\n",
       " u'93',\n",
       " u'9373',\n",
       " u'94',\n",
       " u'95',\n",
       " u'96',\n",
       " u'97',\n",
       " u'98',\n",
       " u'99',\n",
       " u'abandoning',\n",
       " u'ability',\n",
       " u'able',\n",
       " u'abnormal',\n",
       " u'abnormals',\n",
       " u'about',\n",
       " u'above',\n",
       " u'abroad',\n",
       " u'accelerate',\n",
       " u'accept',\n",
       " u'acceptance',\n",
       " u'acceptances',\n",
       " u'accepted',\n",
       " u'accepting',\n",
       " u'access',\n",
       " u'accord',\n",
       " u'according',\n",
       " u'account',\n",
       " u'accountants',\n",
       " u'accounting',\n",
       " u'accounts',\n",
       " u'accused',\n",
       " u'achieve',\n",
       " u'achieved',\n",
       " u'acknowledged',\n",
       " u'acquire',\n",
       " u'acquired',\n",
       " u'acquisition',\n",
       " u'acquisitions',\n",
       " u'across',\n",
       " u'act',\n",
       " u'acting',\n",
       " u'action',\n",
       " u'actions',\n",
       " u'active',\n",
       " u'actively',\n",
       " u'activist',\n",
       " u'activists',\n",
       " u'activities',\n",
       " u'activity',\n",
       " u'actual',\n",
       " u'actually',\n",
       " u'add',\n",
       " u'added',\n",
       " u'adding',\n",
       " u'addition',\n",
       " u'additional',\n",
       " u'address',\n",
       " u'addressed',\n",
       " u'addresses',\n",
       " u'adelaide',\n",
       " u'adequate',\n",
       " u'administration',\n",
       " u'administrative',\n",
       " u'admitted',\n",
       " u'adopt',\n",
       " u'adopted',\n",
       " u'advanced',\n",
       " u'advantage',\n",
       " u'advertising',\n",
       " u'advice',\n",
       " u'advised',\n",
       " u'adviser',\n",
       " u'affairs',\n",
       " u'affect',\n",
       " u'affected',\n",
       " u'affecting',\n",
       " u'affiliate',\n",
       " u'affiliates',\n",
       " u'afford',\n",
       " u'africa',\n",
       " u'african',\n",
       " u'after',\n",
       " u'afternoon',\n",
       " u'again',\n",
       " u'against',\n",
       " u'age',\n",
       " u'agencies',\n",
       " u'agency',\n",
       " u'agf',\n",
       " u'aggressive',\n",
       " u'aggressively',\n",
       " u'ago',\n",
       " u'agree',\n",
       " u'agreed',\n",
       " u'agreement',\n",
       " u'agreements',\n",
       " u'ahead',\n",
       " u'aim',\n",
       " u'aimed',\n",
       " u'aiming',\n",
       " u'aims',\n",
       " u'air',\n",
       " u'aircraft',\n",
       " u'airline',\n",
       " u'airlines',\n",
       " u'airport',\n",
       " u'alan',\n",
       " u'alex',\n",
       " u'all',\n",
       " u'alleged',\n",
       " u'alliance',\n",
       " u'allow',\n",
       " u'allowed',\n",
       " u'allowing',\n",
       " u'allows',\n",
       " u'almost',\n",
       " u'alone',\n",
       " u'along',\n",
       " u'already',\n",
       " u'also',\n",
       " u'alternative',\n",
       " u'although',\n",
       " u'always',\n",
       " u'am',\n",
       " u'america',\n",
       " u'american',\n",
       " u'amid',\n",
       " u'among',\n",
       " u'amount',\n",
       " u'amp',\n",
       " u'an',\n",
       " u'analysis',\n",
       " u'analyst',\n",
       " u'analysts',\n",
       " u'and',\n",
       " u'andrew',\n",
       " u'angeles',\n",
       " u'anniversary',\n",
       " u'announce',\n",
       " u'announced',\n",
       " u'announcement',\n",
       " u'announcing',\n",
       " u'annual',\n",
       " u'annually',\n",
       " u'anonymity',\n",
       " u'another',\n",
       " u'anti',\n",
       " u'anticipated',\n",
       " u'any',\n",
       " u'anyone',\n",
       " u'anything',\n",
       " u'apart',\n",
       " u'apparent',\n",
       " u'apparently',\n",
       " u'appeal',\n",
       " u'appealed',\n",
       " u'appeals',\n",
       " u'appear',\n",
       " u'appearance',\n",
       " u'appeared',\n",
       " u'appears',\n",
       " u'apple',\n",
       " u'applied',\n",
       " u'apply',\n",
       " u'appointed',\n",
       " u'approach',\n",
       " u'approached',\n",
       " u'approaches',\n",
       " u'appropriate',\n",
       " u'approval',\n",
       " u'approvals',\n",
       " u'approve',\n",
       " u'approved',\n",
       " u'april',\n",
       " u'are',\n",
       " u'area',\n",
       " u'areas',\n",
       " u'argue',\n",
       " u'argued',\n",
       " u'argument',\n",
       " u'arise',\n",
       " u'arising',\n",
       " u'arm',\n",
       " u'arms',\n",
       " u'army',\n",
       " u'around',\n",
       " u'arrested',\n",
       " u'arrived',\n",
       " u'as',\n",
       " u'asia',\n",
       " u'asian',\n",
       " u'aside',\n",
       " u'asked',\n",
       " u'asking',\n",
       " u'aspects',\n",
       " u'assembled',\n",
       " u'assembly',\n",
       " u'assessment',\n",
       " u'asset',\n",
       " u'assets',\n",
       " u'assigned',\n",
       " u'associated',\n",
       " u'associates',\n",
       " u'association',\n",
       " u'at',\n",
       " u'atlanta',\n",
       " u'atlantic',\n",
       " u'atp',\n",
       " u'attack',\n",
       " u'attempt',\n",
       " u'attempts',\n",
       " u'attend',\n",
       " u'attended',\n",
       " u'attention',\n",
       " u'attitude',\n",
       " u'attorney',\n",
       " u'attract',\n",
       " u'attractive',\n",
       " u'attrition',\n",
       " u'auchard',\n",
       " u'august',\n",
       " u'australia',\n",
       " u'australian',\n",
       " u'authorisation',\n",
       " u'authorities',\n",
       " u'authority',\n",
       " u'authors',\n",
       " u'auto',\n",
       " u'automaker',\n",
       " u'automakers',\n",
       " u'automobile',\n",
       " u'automotive',\n",
       " u'available',\n",
       " u'average',\n",
       " u'avoid',\n",
       " u'awarded',\n",
       " u'aware',\n",
       " u'away',\n",
       " u'baby',\n",
       " u'back',\n",
       " u'backed',\n",
       " u'background',\n",
       " u'backing',\n",
       " u'bad',\n",
       " u'badly',\n",
       " u'bag',\n",
       " u'bags',\n",
       " u'balance',\n",
       " u'baltimore',\n",
       " u'ban',\n",
       " u'bancorp',\n",
       " u'bank',\n",
       " u'banka',\n",
       " u'bankers',\n",
       " u'banking',\n",
       " u'bankruptcy',\n",
       " u'banks',\n",
       " u'banned',\n",
       " u'barclays',\n",
       " u'barely',\n",
       " u'bargainers',\n",
       " u'bargaining',\n",
       " u'barings',\n",
       " u'barriers',\n",
       " u'base',\n",
       " u'baseball',\n",
       " u'based',\n",
       " u'basic',\n",
       " u'basically',\n",
       " u'basis',\n",
       " u'basket',\n",
       " u'battle',\n",
       " u'battling',\n",
       " u'bay',\n",
       " u'be',\n",
       " u'bear',\n",
       " u'bearish',\n",
       " u'beat',\n",
       " u'became',\n",
       " u'because',\n",
       " u'becker',\n",
       " u'become',\n",
       " u'becoming',\n",
       " u'been',\n",
       " u'beer',\n",
       " u'before',\n",
       " u'began',\n",
       " u'begin',\n",
       " u'beginning',\n",
       " u'begun',\n",
       " u'behind',\n",
       " u'beijing',\n",
       " u'being',\n",
       " u'belief',\n",
       " u'believe',\n",
       " u'believed',\n",
       " u'believes',\n",
       " u'bell',\n",
       " u'below',\n",
       " u'beneficiaries',\n",
       " u'benefit',\n",
       " u'benefits',\n",
       " u'berkeley',\n",
       " u'bernstein',\n",
       " u'besides',\n",
       " u'best',\n",
       " u'better',\n",
       " u'between',\n",
       " u'beyond',\n",
       " u'bid',\n",
       " u'bidder',\n",
       " u'bidding',\n",
       " u'big',\n",
       " u'bigger',\n",
       " u'biggest',\n",
       " u'bill',\n",
       " u'billion',\n",
       " u'billions',\n",
       " u'bills',\n",
       " u'bit',\n",
       " u'blamed',\n",
       " u'block',\n",
       " u'blow',\n",
       " u'blows',\n",
       " u'blue',\n",
       " u'board',\n",
       " u'bob',\n",
       " u'bodies',\n",
       " u'body',\n",
       " u'boeing',\n",
       " u'bond',\n",
       " u'bonds',\n",
       " u'bonus',\n",
       " u'book',\n",
       " u'books',\n",
       " u'boom',\n",
       " u'booming',\n",
       " u'boost',\n",
       " u'boosted',\n",
       " u'boosting',\n",
       " u'border',\n",
       " u'boris',\n",
       " u'boss',\n",
       " u'boston',\n",
       " u'bosworth',\n",
       " u'both',\n",
       " u'bottom',\n",
       " u'bought',\n",
       " u'bound',\n",
       " u'bourse',\n",
       " u'branch',\n",
       " u'branches',\n",
       " u'brand',\n",
       " u'brands',\n",
       " u'break',\n",
       " u'brewer',\n",
       " u'breweries',\n",
       " u'brewing',\n",
       " u'brian',\n",
       " u'brief',\n",
       " u'briefing',\n",
       " u'bring',\n",
       " u'bringing',\n",
       " u'brings',\n",
       " u'brisk',\n",
       " u'britain',\n",
       " u'british',\n",
       " u'broad',\n",
       " u'broadcaster',\n",
       " u'broadcasting',\n",
       " u'broader',\n",
       " u'broke',\n",
       " u'broken',\n",
       " u'broker',\n",
       " u'brokerage',\n",
       " u'brokers',\n",
       " u'brothers',\n",
       " u'brought',\n",
       " u'brown',\n",
       " u'bskyb',\n",
       " u'bt',\n",
       " u'bucharest',\n",
       " u'bucket',\n",
       " u'budget',\n",
       " u'budgets',\n",
       " u'buick',\n",
       " u'build',\n",
       " u'building',\n",
       " u'builds',\n",
       " u'built',\n",
       " u'bulk',\n",
       " u'bullish',\n",
       " u'buoyant',\n",
       " u'buoyed',\n",
       " u'bureau',\n",
       " u'burnham',\n",
       " u'burns',\n",
       " u'bus',\n",
       " u'busiest',\n",
       " u'business',\n",
       " u'businesses',\n",
       " u'businessman',\n",
       " u'but',\n",
       " u'buy',\n",
       " u'buyback',\n",
       " u'buyer',\n",
       " u'buyers',\n",
       " u'buying',\n",
       " u'by',\n",
       " u'bzw',\n",
       " u'cabinet',\n",
       " u'cable',\n",
       " u'calendar',\n",
       " u'calif',\n",
       " u'california',\n",
       " u'call',\n",
       " u'called',\n",
       " u'calling',\n",
       " u'calls',\n",
       " u'calm',\n",
       " u'came',\n",
       " u'camera',\n",
       " u'campaign',\n",
       " u'campaigns',\n",
       " u'can',\n",
       " u'canada',\n",
       " u'canadian',\n",
       " u'canadians',\n",
       " u'cancelled',\n",
       " u'candidate',\n",
       " u'candidates',\n",
       " u'cannot',\n",
       " u'cap',\n",
       " u'capable',\n",
       " u'capacity',\n",
       " u'capital',\n",
       " u'car',\n",
       " u'card',\n",
       " u'cards',\n",
       " u'care',\n",
       " u'careful',\n",
       " u'cargo',\n",
       " u'carpet',\n",
       " u'carried',\n",
       " u'carrier',\n",
       " u'carry',\n",
       " u'carrying',\n",
       " u'cars',\n",
       " u'case',\n",
       " u'cases',\n",
       " u'cash',\n",
       " u'catch',\n",
       " u'category',\n",
       " u'caterpillar',\n",
       " u'caught',\n",
       " u'cause',\n",
       " u'caused',\n",
       " u'caution',\n",
       " u'cautioned',\n",
       " u'cautious',\n",
       " u'caw',\n",
       " u'central',\n",
       " u'centre',\n",
       " u'centres',\n",
       " u'cents',\n",
       " u'century',\n",
       " u'ceo',\n",
       " u'certain',\n",
       " u'certainly',\n",
       " u'chain',\n",
       " u'chair',\n",
       " u'chairman',\n",
       " u'chairwoman',\n",
       " u'challenge',\n",
       " u'championship',\n",
       " u'championships',\n",
       " u'chance',\n",
       " u'chances',\n",
       " u'change',\n",
       " u'changed',\n",
       " u'changes',\n",
       " u'changing',\n",
       " u'channel',\n",
       " u'channels',\n",
       " u'chaotic',\n",
       " u'charge',\n",
       " u'charged',\n",
       " u'charges',\n",
       " u'charles',\n",
       " u'charter',\n",
       " u'cheap',\n",
       " u'chen',\n",
       " u'chethan',\n",
       " u'chevrolet',\n",
       " u'chicago',\n",
       " u'chief',\n",
       " u'child',\n",
       " u'children',\n",
       " u'china',\n",
       " u'chinese',\n",
       " u'chip',\n",
       " u'chips',\n",
       " u'choice',\n",
       " u'chosen',\n",
       " u'christmas',\n",
       " u'christopher',\n",
       " u'chrysler',\n",
       " u'cibc',\n",
       " u'circuit',\n",
       " u'circulation',\n",
       " u'circumstances',\n",
       " u'cisco',\n",
       " u'cited',\n",
       " u'citing',\n",
       " u'city',\n",
       " u'civic',\n",
       " u'civil',\n",
       " u'claim',\n",
       " u'claims',\n",
       " u'class',\n",
       " u'classified',\n",
       " u'clause',\n",
       " u'clear',\n",
       " u'clearly',\n",
       " u'clients',\n",
       " u'climate',\n",
       " u'climbed',\n",
       " u'climbing',\n",
       " u'clinton',\n",
       " u'close',\n",
       " u'closed',\n",
       " u'closely',\n",
       " u'closer',\n",
       " u'closing',\n",
       " u'clothing',\n",
       " u'club',\n",
       " u'co',\n",
       " u'coalition',\n",
       " u'cold',\n",
       " u'colin',\n",
       " u'collapse',\n",
       " u'collapsed',\n",
       " u'columbia',\n",
       " u'com',\n",
       " u'combination',\n",
       " u'combine',\n",
       " u'combined',\n",
       " u'come',\n",
       " u'comeback',\n",
       " u'comes',\n",
       " u'comfortable',\n",
       " u'coming',\n",
       " u'commanding',\n",
       " u'comment',\n",
       " u'comments',\n",
       " u'commerce',\n",
       " u'commercial',\n",
       " u'commission',\n",
       " u'commitment',\n",
       " u'committed',\n",
       " u'committee',\n",
       " u'commodity',\n",
       " u'common',\n",
       " u'communications',\n",
       " u'communist',\n",
       " u'communists',\n",
       " u'communities',\n",
       " u'community',\n",
       " u'compact',\n",
       " u'companies',\n",
       " u'company',\n",
       " u'compaq',\n",
       " u'comparable',\n",
       " u'compared',\n",
       " u'compensation',\n",
       " u'compete',\n",
       " u'competing',\n",
       " u'competition',\n",
       " u'competitive',\n",
       " u'competitiveness',\n",
       " u'competitor',\n",
       " u'competitors',\n",
       " u'complained',\n",
       " u'complete',\n",
       " u'completed',\n",
       " u'completely',\n",
       " u'completion',\n",
       " u'complex',\n",
       " u'component',\n",
       " u'components',\n",
       " u'composite',\n",
       " u'comprehensive',\n",
       " u'comptroller',\n",
       " u'compuserve',\n",
       " u'computer',\n",
       " u'computers',\n",
       " u'concentrate',\n",
       " u'concentrated',\n",
       " u'concept',\n",
       " u'concern',\n",
       " u'concerned',\n",
       " u'concerns',\n",
       " u'concluded',\n",
       " u'concrete',\n",
       " u'condition',\n",
       " u'conditions',\n",
       " u'conducted',\n",
       " u'conference',\n",
       " u'confidence',\n",
       " u'confident',\n",
       " u'confirmed',\n",
       " u'conflicting',\n",
       " u'conglomerate',\n",
       " u'conglomerates',\n",
       " u'congress',\n",
       " u'congressional',\n",
       " u'connect',\n",
       " u'conrad',\n",
       " u'consecutive',\n",
       " u'consensus',\n",
       " u'consent',\n",
       " u'conservative',\n",
       " u'conservatives',\n",
       " u'consider',\n",
       " u'considerable',\n",
       " u'considered',\n",
       " u'considering',\n",
       " u'considers',\n",
       " u'consistently',\n",
       " u'consolidated',\n",
       " u'consolidation',\n",
       " u'consortium',\n",
       " u'constituencies',\n",
       " u'construction',\n",
       " u'constructive',\n",
       " u'consultant',\n",
       " u'consulting',\n",
       " u'consumer',\n",
       " u'consumers',\n",
       " u'contact',\n",
       " u'contacted',\n",
       " u'contain',\n",
       " u'containing',\n",
       " u'contend',\n",
       " u'content',\n",
       " u'contentious',\n",
       " u'continental',\n",
       " u'continue',\n",
       " u'continued',\n",
       " u'continues',\n",
       " u'continuing',\n",
       " u'contract',\n",
       " u'contractor',\n",
       " u'contractors',\n",
       " u'contracts',\n",
       " u'contrast',\n",
       " u'contributed',\n",
       " u'contribution',\n",
       " u'control',\n",
       " u'controlled',\n",
       " u'controls',\n",
       " u'controversial',\n",
       " u'controversy',\n",
       " u'convict',\n",
       " u'convicted',\n",
       " u'conviction',\n",
       " u'cooperation',\n",
       " u'copyright',\n",
       " u'core',\n",
       " u'corp',\n",
       " u'corporate',\n",
       " u'correction',\n",
       " u'corruption',\n",
       " u'cos',\n",
       " u'cost',\n",
       " u'costly',\n",
       " u'costs',\n",
       " u'could',\n",
       " u'council',\n",
       " u'counsel',\n",
       " u'counter',\n",
       " u'counterparts',\n",
       " u'countries',\n",
       " u'country',\n",
       " u'couple',\n",
       " u'course',\n",
       " u'court',\n",
       " u'courts',\n",
       " u'cover',\n",
       " u'covered',\n",
       " u'covering',\n",
       " u'covers',\n",
       " u'crack',\n",
       " u'cracked',\n",
       " u'crash',\n",
       " u'create',\n",
       " u'created',\n",
       " u'creating',\n",
       " u'creation',\n",
       " u'credit',\n",
       " u'crew',\n",
       " u'crime',\n",
       " u'crimes',\n",
       " u'crisis',\n",
       " u'critical',\n",
       " u'criticised',\n",
       " u'criticism',\n",
       " u'critics',\n",
       " u'cross',\n",
       " u'crown',\n",
       " u'crowns',\n",
       " u'crucial',\n",
       " u'crushed',\n",
       " u'cs',\n",
       " u'csu',\n",
       " u'cultural',\n",
       " u'culture',\n",
       " u'cup',\n",
       " u'curbs',\n",
       " u'currency',\n",
       " u'current',\n",
       " u'currently',\n",
       " u'customer',\n",
       " u'customers',\n",
       " u'cut',\n",
       " u'cuts',\n",
       " u'cutting',\n",
       " u'cyberspace',\n",
       " u'cycle',\n",
       " u'czech',\n",
       " u'czechs',\n",
       " u'daily',\n",
       " u'dain',\n",
       " u'damage',\n",
       " u'damaged',\n",
       " u'damaging',\n",
       " u'damm',\n",
       " u'dampen',\n",
       " u'dan',\n",
       " u'danger',\n",
       " u'daring',\n",
       " u'data',\n",
       " u'date',\n",
       " u'david',\n",
       " u'davis',\n",
       " u'day',\n",
       " u'days',\n",
       " u'dayton',\n",
       " u'de',\n",
       " u'deadline',\n",
       " u'deal',\n",
       " u'dealer',\n",
       " u'dealers',\n",
       " u'dealing',\n",
       " u'deals',\n",
       " u'dealt',\n",
       " u'dean',\n",
       " u'death',\n",
       " u'debate',\n",
       " u'debt',\n",
       " u'debts',\n",
       " u'dec',\n",
       " u'decade',\n",
       " u'decades',\n",
       " u'december',\n",
       " u'decide',\n",
       " u'decided',\n",
       " u'decision',\n",
       " u'decisions',\n",
       " u'declared',\n",
       " u'decline',\n",
       " u'declined',\n",
       " u'declines',\n",
       " u'declining',\n",
       " u'decode',\n",
       " u'deep',\n",
       " u'defeat',\n",
       " u'defeated',\n",
       " u'defence',\n",
       " u'defend',\n",
       " u'defended',\n",
       " u'deficit',\n",
       " u'definitely',\n",
       " u'defying',\n",
       " u'degree',\n",
       " u'delay',\n",
       " u'delays',\n",
       " u'deliver',\n",
       " u'delivered',\n",
       " u'delivery',\n",
       " u'demand',\n",
       " u'demanded',\n",
       " u'demands',\n",
       " u'demerger',\n",
       " u'democracy',\n",
       " u'democrat',\n",
       " u'democratic',\n",
       " u'democrats',\n",
       " u'demonstrated',\n",
       " u'demonstrations',\n",
       " u'deng',\n",
       " u'denied',\n",
       " u'denmark',\n",
       " u'dennis',\n",
       " u'department',\n",
       " u'departure',\n",
       " u'depends',\n",
       " u'deputy',\n",
       " u'derivatives',\n",
       " u'described',\n",
       " u'design',\n",
       " u'designed',\n",
       " u'desire',\n",
       " u'despite',\n",
       " u'detailed',\n",
       " u'details',\n",
       " u'detained',\n",
       " u'detention',\n",
       " u'determined',\n",
       " u'detroit',\n",
       " u'deutsche',\n",
       " u'develop',\n",
       " u'developed',\n",
       " u'developing',\n",
       " u'development',\n",
       " u'developments',\n",
       " u'devices',\n",
       " u'did',\n",
       " u'didn',\n",
       " u'died',\n",
       " u'diego',\n",
       " u'difference',\n",
       " u'differences',\n",
       " u'different',\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "trainX =np.array([])\n",
    "labels = []\n",
    "path = '../data/C50train/'\n",
    "authors = os.listdir(path)[:10]; \n",
    "for auth in authors:\n",
    "    files = os.listdir(path+auth+'/');\n",
    "    tmpX,tmpY=np.array([]),[]\n",
    "    for file in files:\n",
    "        f=open(path+auth+'/'+file, 'r')\n",
    "        data = f.read().replace('\\n', ' ')\n",
    "        tmpX=np.append(tmpX,data)\n",
    "        tmpY=tmpY+[auth]\n",
    "        f.close()\n",
    "    trainX=np.append(trainX, tmpX)\n",
    "    labels=labels+tmpY \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainX, labels, train_size = 0.8)\n",
    "\n",
    "import pandas as pd\n",
    "X_train = pd.Series(X_train)\n",
    "X_test = pd.Series(X_test)\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect.fit(X_train)\n",
    "\n",
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform training data into a 'document-term matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<400x3527 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 87335 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors = vect.fit_transform(X_train)\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0003</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>0700</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yuan</th>\n",
       "      <th>yukon</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zedong</th>\n",
       "      <th>zemin</th>\n",
       "      <th>zivnostenska</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 3527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000  0003   01        02   04   05   06   07  0700      ...       \\\n",
       "0  0.0  0.052703   0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0      ...        \n",
       "1  0.0  0.000000   0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0      ...        \n",
       "2  0.0  0.012975   0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0      ...        \n",
       "3  0.0  0.000000   0.0  0.0  0.033378  0.0  0.0  0.0  0.0   0.0      ...        \n",
       "\n",
       "   younger  youngest  your  yourself  yuan  yukon  zealand  zedong  zemin  \\\n",
       "0      0.0       0.0   0.0       0.0   0.0    0.0      0.0     0.0    0.0   \n",
       "1      0.0       0.0   0.0       0.0   0.0    0.0      0.0     0.0    0.0   \n",
       "2      0.0       0.0   0.0       0.0   0.0    0.0      0.0     0.0    0.0   \n",
       "3      0.0       0.0   0.0       0.0   0.0    0.0      0.0     0.0    0.0   \n",
       "\n",
       "   zivnostenska  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "\n",
       "[4 rows x 3527 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_vectors.toarray(), columns=vect.get_feature_names()).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Build our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(train_vectors.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "   AaronPressman       0.97      0.93      0.95        40\n",
      "      AlanCrosby       1.00      0.94      0.97        36\n",
      "  AlexanderSmith       0.82      0.95      0.88        42\n",
      " BenjaminKangLim       1.00      0.95      0.97        37\n",
      "   BernardHickey       0.91      0.76      0.83        41\n",
      "     BradDorfman       0.86      0.78      0.82        40\n",
      "DarrenSchuettler       0.71      0.92      0.80        39\n",
      "     DavidLawder       0.93      0.90      0.92        42\n",
      "   EdnaFernandes       0.91      0.91      0.91        44\n",
      "     EricAuchard       0.97      0.95      0.96        39\n",
      "\n",
      "     avg / total       0.91      0.90      0.90       400\n",
      "\n",
      "ACCURACY:: 0.8975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.classification import classification_report, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "# generate your cross-validation prediction with 10 fold Stratified sampling\n",
    "y_pred = cross_val_predict(nb, train_vectors.toarray(), y_train, cv=10)\n",
    "\n",
    "print(classification_report(y_train, y_pred))\n",
    "print \"ACCURACY::\",accuracy_score(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is all about unstructured data, and one of the problem industry is facing today is about amount of data that any System has to process. Often its not practical to read through a huge volume of data and get some insights about that data. Consider google news, there are hundred of thousands of news get published on daily basis. So we need a way to group news with some keywords in order to understand what is going on. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/topic_modelling.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One in red are classes, which are fixed and with the help of training data, we can build news classifier.\n",
    "- But one in green are topics, that are identified run time. And process of identification of topics is totally unsupervised. And Topic modelling is one the best way to understand, repersent any unstructured text without actually getting into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Topic Modelling__ as the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n",
    "\n",
    "A __Topic__ can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document Clustering.\n",
    "    1. Group news.\n",
    "    2. Group emails.\n",
    "    3. Group similar medical notes etc.\n",
    "- Keywords Generation. Can be used for SEO.\n",
    "- Build WordCloud.\n",
    "- Build Search Engines.\n",
    "- Build knowledge-graph(aka ontologies).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Topic Modelling Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are generally important words in text. \n",
    "- Frequency count can be one of the way to identify topics.\n",
    "- TF-IDF can also be used for Topic Modelling.\n",
    "- Or most famous, LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have the following set of sentences:\n",
    "\n",
    "- I like to eat broccoli and bananas.\n",
    "- I ate a banana and spinach smoothie for breakfast.\n",
    "- Chinchillas and kittens are cute.\n",
    "- My sister adopted a kitten yesterday.\n",
    "- Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "LDA will try to identify words which have been used in similar context and will calculate probability of occuring two words togther.\n",
    "In the above example, LDA will create topics like:\n",
    "    \n",
    "- Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
    "- Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim(https://radimrehurek.com/gensim/) package in python implements most of topic modelling algorithms.\n",
    "\n",
    "* We'll walk through a basic application of Topic Modeling with LDA\n",
    "* We'll also cover the basic NLP operations necessary for the application\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"./data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "# compile documents\n",
    "doc_complete =sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fast-forward through pre-processing\n",
    "\n",
    "* After the processing, we'll have *texts* - a tokenized, stopped and stemmed list of words from a single document\n",
    "* Let’s fast forward and loop through all our documents and appended each one to *texts*\n",
    "* So now *texts* is a list of lists, one list for each of our original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### texts\n",
      "[['break', 'u', u'justic', u'depart', 'world', 'wide', 'web', 'site', 'last', 'week', u'highlight', 'internet', u'continu', u'vulner', u'hacker'], [u'unidentifi', u'hacker', u'gain', u'access', u'depart', 'web', 'page', 'august', '16', u'replac', 'hate', u'fill', u'diatrib', u'label', u'depart', u'injustic', u'includ', 'swastika', u'pictur', 'adolf', 'hitler'], [u'justic', u'offici', u'quickli', u'pull', 'plug', u'vandalis', 'page', u'secur', u'flaw', u'allow', u'hacker', 'gain', u'entri', u'like', 'exist', u'thousand', u'corpor', u'govern', 'web', u'site', u'secur', u'expert', 'said'], ['vast', u'major', u'site', u'vulner', 'said', 'richard', 'power', 'senior', 'analyst', u'comput', u'secur', u'institut'], [u'justic', u'depart', u'singl'], [u'justic', u'depart', u'offici', 'said', u'compromis', 'web', 'site', u'connect', u'comput', u'contain', u'sensit', u'file'], ['web', 'site', 'http', 'www', 'usdoj', 'gov', u'includ', u'copi', u'press', u'releas', u'speech', u'publicli', u'avail', u'inform'], [u'secur', 'breach', 'like', 'graffiti', u'outsid', u'build', 'spokesman', 'bert', 'brandenburg', 'said'], [u'organis', u'target', 'past'], ['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', u'vandalis'], [u'hacker', 'make', '250', '000', u'attempt', u'annual', 'break', 'u', u'militari', u'comput', u'accord', u'gener', u'account', u'offic', 'report'], [u'window', u'magazin', u'recent', 'found', u'secur', u'flaw', 'web', u'site', 'dozen', 'major', u'corpor'], ['web', u'spectacularli', u'insecur', 'editor', 'mike', 'elgan', 'said'], [u'reli', u'secur', u'hole', u'document', u'softwar', u'manufactur', u'month', 'earlier', u'magazin', u'specialist', u'abl', 'gain', u'variou', u'degre', u'unauthoris', u'access', u'differ', u'site'], ['elgan', 'said', u'hacker', u'exploit', u'flaw', u'motiv', 'anger', 'growth', u'commerci', 'internet'], ['common', 'theme', u'hacker', 'fed', 'non', u'hacker', 'internet', 'said'], [u'battl', u'complet', u'hopeless'], [u'secur', 'web', 'site', 'richard', 'power', 'said'], [u'kind', u'measur', 'take'], [u'corpor', u'institut', 'take', u'simpli', u'noth', 'bad', u'happen', 'yet'], [u'site', u'use', u'multipl', u'layer', u'secur', 'well', 'beyond', u'simpl', 'password', u'protect', 'keep', u'hacker'], ['one', 'site', u'mention', u'window', u'magazin', u'fidel', u'invest'], [u'fidel', 'site', u'advertis', 'mutual', u'fund', u'dissemin', u'inform', u'person', u'financ', 'contain', u'confidenti', u'custom', u'inform'], [u'fidel', u'offici', u'immedi', u'close', u'loophol', u'identifi', u'magazin', 'spokeswoman', 'said'], [u'multipl', u'secur', u'measur', u'previous', 'place', 'would', u'prevent', u'secur', 'breach', u'despit', 'hole', 'spokeswoman', u'ad']]\n",
      "\n",
      "##### The lines in texts\n",
      "['break', 'u', u'justic', u'depart', 'world', 'wide', 'web', 'site', 'last', 'week', u'highlight', 'internet', u'continu', u'vulner', u'hacker']\n",
      "[u'unidentifi', u'hacker', u'gain', u'access', u'depart', 'web', 'page', 'august', '16', u'replac', 'hate', u'fill', u'diatrib', u'label', u'depart', u'injustic', u'includ', 'swastika', u'pictur', 'adolf', 'hitler']\n",
      "[u'justic', u'offici', u'quickli', u'pull', 'plug', u'vandalis', 'page', u'secur', u'flaw', u'allow', u'hacker', 'gain', u'entri', u'like', 'exist', u'thousand', u'corpor', u'govern', 'web', u'site', u'secur', u'expert', 'said']\n",
      "['vast', u'major', u'site', u'vulner', 'said', 'richard', 'power', 'senior', 'analyst', u'comput', u'secur', u'institut']\n",
      "[u'justic', u'depart', u'singl']\n",
      "[u'justic', u'depart', u'offici', 'said', u'compromis', 'web', 'site', u'connect', u'comput', u'contain', u'sensit', u'file']\n",
      "['web', 'site', 'http', 'www', 'usdoj', 'gov', u'includ', u'copi', u'press', u'releas', u'speech', u'publicli', u'avail', u'inform']\n",
      "[u'secur', 'breach', 'like', 'graffiti', u'outsid', u'build', 'spokesman', 'bert', 'brandenburg', 'said']\n",
      "[u'organis', u'target', 'past']\n",
      "['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', u'vandalis']\n",
      "[u'hacker', 'make', '250', '000', u'attempt', u'annual', 'break', 'u', u'militari', u'comput', u'accord', u'gener', u'account', u'offic', 'report']\n",
      "[u'window', u'magazin', u'recent', 'found', u'secur', u'flaw', 'web', u'site', 'dozen', 'major', u'corpor']\n",
      "['web', u'spectacularli', u'insecur', 'editor', 'mike', 'elgan', 'said']\n",
      "[u'reli', u'secur', u'hole', u'document', u'softwar', u'manufactur', u'month', 'earlier', u'magazin', u'specialist', u'abl', 'gain', u'variou', u'degre', u'unauthoris', u'access', u'differ', u'site']\n",
      "['elgan', 'said', u'hacker', u'exploit', u'flaw', u'motiv', 'anger', 'growth', u'commerci', 'internet']\n",
      "['common', 'theme', u'hacker', 'fed', 'non', u'hacker', 'internet', 'said']\n",
      "[u'battl', u'complet', u'hopeless']\n",
      "[u'secur', 'web', 'site', 'richard', 'power', 'said']\n",
      "[u'kind', u'measur', 'take']\n",
      "[u'corpor', u'institut', 'take', u'simpli', u'noth', 'bad', u'happen', 'yet']\n",
      "[u'site', u'use', u'multipl', u'layer', u'secur', 'well', 'beyond', u'simpl', 'password', u'protect', 'keep', u'hacker']\n",
      "['one', 'site', u'mention', u'window', u'magazin', u'fidel', u'invest']\n",
      "[u'fidel', 'site', u'advertis', 'mutual', u'fund', u'dissemin', u'inform', u'person', u'financ', 'contain', u'confidenti', u'custom', u'inform']\n",
      "[u'fidel', u'offici', u'immedi', u'close', u'loophol', u'identifi', u'magazin', 'spokeswoman', 'said']\n",
      "[u'multipl', u'secur', u'measur', u'previous', 'place', 'would', u'prevent', u'secur', 'breach', u'despit', 'hole', 'spokeswoman', u'ad']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = sents\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "print(\"\\n##### texts\")\n",
    "print(texts)\n",
    "\n",
    "print(\"\\n##### The lines in texts\")\n",
    "for line in texts:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "* To generate an LDA model, we need to understand how frequently each term occurs within each document\n",
    "* To do that, we need to construct a document-term matrix with a package called *gensim*\n",
    "\n",
    "# Topic Modeling with gensim\n",
    "\n",
    "## Getting started with gensim?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(175 unique tokens: [u'theme', u'identifi', u'offici', u'month', u'report']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics\n",
    "* To see each token’s unique integer id, try -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'theme': 129, u'identifi': 167, u'offici': 40, u'month': 117, u'report': 97, u'simpli': 139, u'hate': 28, u'yet': 140, u'islam': 88, u'web': 4, u'adolf': 29, u'sensit': 58, u'manufactur': 118, u'offic': 95, u'breach': 79, u'vulner': 13, u'swastika': 27, u'take': 137, u'non': 130, u'march': 83, u'variou': 112, u'wide': 1, u'financ': 159, u'kind': 136, u'nation': 85, u'break': 9, u'mention': 154, u'govern': 45, u'press': 64, u'world': 3, u'password': 150, u'vast': 55, u'measur': 135, u'gov': 72, u'like': 36, u'corpor': 41, u'elgan': 107, u'magazin': 101, u'editor': 109, u'contain': 61, u'found': 100, u'page': 21, u'prevent': 172, u'www': 62, u'replac': 19, u'continu': 8, u'past': 80, u'growth': 125, u'connect': 59, u'year': 86, u'close': 165, u'happen': 141, u'beyond': 144, u'hacker': 14, u'said': 35, u'expert': 37, u'spokesman': 75, u'abl': 115, u'bad': 138, u'label': 22, u'access': 23, u'use': 143, u'internet': 11, u'injustic': 15, u'insecur': 105, u'common': 131, u'specialist': 113, u'accord': 94, u'confidenti': 157, u'august': 17, u'power': 50, u'despit': 169, u'gener': 99, u'million': 84, u'fed': 128, u'vandalis': 42, u'unauthoris': 120, u'highlight': 6, u'releas': 71, u'advertis': 162, u'plug': 32, u'protect': 147, u'last': 12, u'justic': 5, u'militari': 91, u'annual': 93, u'keep': 149, u'degre': 116, u'000': 96, u'organis': 81, u'hopeless': 133, u'outsid': 78, u'compromis': 57, u'softwar': 110, u'major': 48, u'depart': 2, u'usdoj': 66, u'one': 153, u'exploit': 124, u'250': 98, u'hitler': 30, u'ad': 170, u'differ': 111, u'institut': 49, u'would': 171, u'pictur': 18, u'window': 103, u'custom': 160, u'avail': 67, u'reli': 119, u'includ': 25, u'recent': 102, u'quickli': 47, u'bert': 73, u'flaw': 34, u'thousand': 46, u'copi': 65, u'fund': 158, u'gain': 31, u'hole': 121, u'analyst': 54, u'motiv': 126, u'pull': 33, u'account': 89, u'immedi': 166, u'target': 82, u'16': 20, u'battl': 132, u'commerci': 127, u'spokeswoman': 164, u'publicli': 70, u'invest': 152, u'exist': 43, u'dissemin': 163, u'document': 122, u'layer': 148, u'comput': 52, u'person': 161, u'site': 7, u'graffiti': 76, u'file': 60, u'anger': 123, u'fill': 16, u'mutual': 156, u'multipl': 145, u'secur': 38, u'make': 92, u'brandenburg': 74, u'mike': 106, u'speech': 69, u'build': 77, u'diatrib': 26, u'place': 173, u'dozen': 104, u'simpl': 151, u'noth': 142, u'singl': 56, u'complet': 134, u'week': 0, u'http': 63, u'spectacularli': 108, u'fidel': 155, u'earlier': 114, u'unidentifi': 24, u'previous': 174, u'loophol': 168, u'man': 87, u'attempt': 90, u'richard': 51, u'entri': 39, u'well': 146, u'inform': 68, u'u': 10, u'allow': 44, u'senior': 53}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our dictionary must be converted into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(2, 2), (4, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(4, 1), (5, 1), (7, 1), (14, 1), (21, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]\n",
      "[(7, 1), (13, 1), (35, 1), (38, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)]\n",
      "[(2, 1), (5, 1), (56, 1)]\n",
      "[(2, 1), (4, 1), (5, 1), (7, 1), (35, 1), (40, 1), (52, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)]\n",
      "[(4, 1), (7, 1), (25, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1)]\n",
      "[(35, 1), (36, 1), (38, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1)]\n",
      "[(80, 1), (81, 1), (82, 1)]\n",
      "[(4, 1), (7, 1), (12, 1), (42, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1)]\n",
      "[(9, 1), (10, 1), (14, 1), (52, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1)]\n",
      "[(4, 1), (7, 1), (34, 1), (38, 1), (41, 1), (48, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1)]\n",
      "[(4, 1), (35, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1)]\n",
      "[(7, 1), (23, 1), (31, 1), (38, 1), (101, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)]\n",
      "[(11, 1), (14, 1), (34, 1), (35, 1), (107, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1)]\n",
      "[(11, 1), (14, 2), (35, 1), (128, 1), (129, 1), (130, 1), (131, 1)]\n",
      "[(132, 1), (133, 1), (134, 1)]\n",
      "[(4, 1), (7, 1), (35, 1), (38, 1), (50, 1), (51, 1)]\n",
      "[(135, 1), (136, 1), (137, 1)]\n",
      "[(41, 1), (49, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1)]\n",
      "[(7, 1), (14, 1), (38, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1)]\n",
      "[(7, 1), (101, 1), (103, 1), (152, 1), (153, 1), (154, 1), (155, 1)]\n",
      "[(7, 1), (61, 1), (68, 2), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1)]\n",
      "[(35, 1), (40, 1), (101, 1), (155, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1)]\n",
      "[(38, 2), (79, 1), (121, 1), (135, 1), (145, 1), (164, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The doc2bow() function converts dictionary into a bag-of-words\n",
    "* The result, *corpus*, is a list of vectors equal to the number of documents\n",
    "* In each document vector is a series of tuples\n",
    "* The tuples are (term ID, term frequency) pairs\n",
    "* This includes terms that actually occur - terms that do not occur in a document will not appear in that document’s vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Quiz\n",
    "\n",
    "Looking at the data above, please answer the following:\n",
    "* How many times does *basebal* occur in *doc_a*?\n",
    "* How many times does *basebal* occur in *doc_b*?\n",
    "* How many times does *health* occur in *doc_e*?\n",
    "* Give an example of a word that occurs in *doc_a* but doesn't occur in *doc_b*.\n",
    "* How many times does *brother* occur in all the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LDA Model\n",
    "\n",
    "*corpus* is a (sparse) document-term matrix and now we’re ready to generate an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to the LDA model\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "* num_topics\n",
    "    - required\n",
    "    - An LDA model requires the user to determine how many topics should be generated\n",
    "    - Our document set is small, so we’re only asking for three topics\n",
    "* id2word\n",
    "    - required\n",
    "    - The LdaModel class requires our previous dictionary to map ids to strings\n",
    "* passes\n",
    "    - optional\n",
    "    - The number of laps the model will take through corpus\n",
    "    - The greater the number of passes, the more accurate the model will be\n",
    "    - A lot of passes can be slow on a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=3, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\" + 0.023*\"hacker\" + 0.023*\"depart\" + 0.017*\"comput\" + 0.017*\"said\" + 0.017*\"justic\" + 0.012*\"offici\" + 0.012*\"gain\"'), (1, u'0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\" + 0.025*\"internet\" + 0.025*\"web\" + 0.025*\"secur\" + 0.018*\"flaw\" + 0.018*\"window\" + 0.018*\"magazin\" + 0.010*\"vulner\"'), (2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\" + 0.020*\"web\" + 0.020*\"said\" + 0.011*\"fidel\" + 0.011*\"hole\" + 0.011*\"institut\" + 0.011*\"spokeswoman\" + 0.011*\"access\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\" + 0.020*\"web\" + 0.020*\"said\" + 0.011*\"fidel\" + 0.011*\"hole\" + 0.011*\"institut\" + 0.011*\"spokeswoman\" + 0.011*\"access\"')\n",
      "(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\" + 0.023*\"hacker\" + 0.023*\"depart\" + 0.017*\"comput\" + 0.017*\"said\" + 0.017*\"justic\" + 0.012*\"offici\" + 0.012*\"gain\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=2):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\"')\n",
      "(1, u'0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\"')\n",
      "(2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Within each topic are the three most probable words to appear in that topic\n",
    "\n",
    "## Topics in detail\n",
    "Let's now look at a topic in detail. Let us see how distinct the topics are, and if they seem to capture any context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\" + 0.023*\"hacker\" + 0.023*\"depart\" + 0.017*\"comput\" + 0.017*\"said\" + 0.017*\"justic\" + 0.012*\"offici\" + 0.012*\"gain\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\" + 0.025*\"internet\" + 0.025*\"web\" + 0.025*\"secur\" + 0.018*\"flaw\" + 0.018*\"window\" + 0.018*\"magazin\" + 0.010*\"vulner\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\" + 0.020*\"web\" + 0.020*\"said\" + 0.011*\"fidel\" + 0.011*\"hole\" + 0.011*\"institut\" + 0.011*\"spokeswoman\" + 0.011*\"access\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the topics make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\"')\n",
      "(1, u'0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\"')\n",
      "(2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the model\n",
    "\n",
    "Two topics seems like a better fit for our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.037*\"secur\" + 0.032*\"site\" + 0.018*\"magazin\" + 0.017*\"said\"')\n",
      "(1, u'0.028*\"web\" + 0.028*\"said\" + 0.027*\"hacker\" + 0.027*\"site\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with more passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.023*\"site\" + 0.023*\"web\" + 0.017*\"take\" + 0.010*\"hole\"')\n",
      "(1, u'0.033*\"secur\" + 0.033*\"site\" + 0.030*\"said\" + 0.029*\"hacker\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=200)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Topic for new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_f = \"Are Health professionals justified in saying that brocolli is good for your health?\" \n",
    "\n",
    "doc_set = [doc_f]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "infer = ldamodel[corpus[0]]\n",
    "\n",
    "# https://radimrehurek.com/gensim/wiki.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=2, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "#Lets check by default LDA parameters\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Tree Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advance topic in NLP is Lexical Analysis of text wherein we try to analyze and understand text. This process is called deep tree parsing in NLP world where we try to analyze relationships amongst the text.\n",
    "- Text parsing is important when you want to know relationships in text. For example <i>Delhi is capital of India<i>, here Delhi and India are related and having a relationship <b>is capital of<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "sent = \"the dog saw a man in the park\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../images/deep_parsing.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well we have to define our grammar, which looks quite tedious job. But there are other NLP packages such as Stanford CoreNLP which provides funcitons to generate parse tree from unstructured text without defining any grammar.\n",
    "- Parse tree provides us meaningful and true relations and also kind of relations they share. Also called facts.\n",
    "- Tree Parsing is used to build knowledge base from unstructured corpus. Check DbPedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# QUIZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

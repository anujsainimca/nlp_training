{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  A sub-field of Artificial Intelligence (AI).\n",
    "-  Aim is to build intelligent computers that can interact with human biengs like human beings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuter_50_50 Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AaronPressman',\n",
       " 'AlanCrosby',\n",
       " 'AlexanderSmith',\n",
       " 'BenjaminKangLim',\n",
       " 'BernardHickey',\n",
       " 'BradDorfman',\n",
       " 'DarrenSchuettler',\n",
       " 'DavidLawder',\n",
       " 'EdnaFernandes',\n",
       " 'EricAuchard',\n",
       " 'FumikoFujisaki',\n",
       " 'GrahamEarnshaw',\n",
       " 'HeatherScoffield',\n",
       " 'JaneMacartney',\n",
       " 'JanLopatka',\n",
       " 'JimGilchrist',\n",
       " 'JoeOrtiz',\n",
       " 'JohnMastrini',\n",
       " 'JonathanBirt',\n",
       " 'JoWinterbottom',\n",
       " 'KarlPenhaul',\n",
       " 'KeithWeir',\n",
       " 'KevinDrawbaugh',\n",
       " 'KevinMorrison',\n",
       " 'KirstinRidley',\n",
       " 'KouroshKarimkhany',\n",
       " 'LydiaZajc',\n",
       " \"LynneO'Donnell\",\n",
       " 'LynnleyBrowning',\n",
       " 'MarcelMichelson',\n",
       " 'MarkBendeich',\n",
       " 'MartinWolk',\n",
       " 'MatthewBunce',\n",
       " 'MichaelConnor',\n",
       " 'MureDickie',\n",
       " 'NickLouth',\n",
       " 'PatriciaCommins',\n",
       " 'PeterHumphrey',\n",
       " 'PierreTran',\n",
       " 'RobinSidel',\n",
       " 'RogerFillion',\n",
       " 'SamuelPerry',\n",
       " 'SarahDavison',\n",
       " 'ScottHillis',\n",
       " 'SimonCowell',\n",
       " 'TanEeLyn',\n",
       " 'TheresePoletti',\n",
       " 'TimFarrand',\n",
       " 'ToddNissen',\n",
       " 'WilliamKazer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"./data/C50train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are authors' names and we have got 50 articlesfrom each author. So one of the problem that we can solve using this data is to identify author by reading his articles. Generally, as a writer, we all follow certain style; things like kind of words we use, how lenghty sentences we write, how do we use punctuations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A break-in at the U.S. Justice Department\\'s World Wide Web site last week highlighted the Internet\\'s continued vulnerability to hackers.\\nUnidentified hackers gained access to the department\\'s web page on August 16 and replaced it with a hate-filled diatribe labelled the \"Department of Injustice\" that included a swastika and a picture of Adolf Hitler.\\nJustice officials quickly pulled the plug on the vandalised page, but the security flaws that allowed hackers to gain entry likely exist in thousands of other corporate and government web sites, security experts said.\\n\"The vast majority of sites are vulnerable,\" said Richard Power, senior analyst at the Computer Security Institute. \"The Justice Department shouldn\\'t be singled out.\"\\nJustice Department officials said the compromised web site was not connected to any computers containing sensitive files. The web site (http://www.usdoj.gov) included copies of press releases, speeches and other publicly available information.\\nThe security breach \"is just like graffiti on the outside of the building,\" spokesman Bert Brandenburg said.\\nOther organisations have been targeted in the past. Last year, the Nation of Islam\\'s Million Man March web site was vandalised. And hackers make 250,000 attempts annually to break into U.S. military computers, according to a General Accounting Office report.\\nWindows Magazine recently found security flaws at web sites of a dozen major corporations. \"The web is spectacularly insecure,\" editor Mike Elgan said. Relying on security holes that had been documented by software manufacturers months earlier, the magazine\\'s specialists were able to gain various degrees of unauthorised access at the different sites.\\nElgan said hackers who are exploiting some of the same flaws are motivated by anger over the growth and commercialization of the Internet. \"A common theme is that hackers are fed up with non-hackers on the Internet,\" he said.\\nThe battle is not completely hopeless. \"You can secure a web site,\" Richard Power said. \"There\\'s all kinds of measures you can take. Most corporations and institutions don\\'t take them simply because nothing bad has happened to them yet.\"\\nSome sites are using multiple layers of security, well beyond simple password protection, to keep hackers out.\\nOne site mentioned by Windows Magazine was Fidelity Investments. Fidelity\\'s site advertises its mutual funds and disseminates information about personal finance but does not contain confidential customer information.\\nFidelity officials immediately closed the loophole identified by the magazine, a spokeswoman said. But multiple security measures previously in place would have prevented a security breach despite the hole, the spokeswoman added.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets look into of the article by one of the author and experiment with some NLP tasks\n",
    "f=open(\"./data/C50train/AaronPressman/2537newsML.txt\")\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Sentence Splitter\n",
    "-  Tokenization\n",
    "-  POS (part of speech tagging)\n",
    "-  Stop-word Removal\n",
    "-  Stemming\n",
    "-  Lemmatization\n",
    "-  Chunking\n",
    "-  Deep Parsing\n",
    "-  Word Cloud\n",
    "-  TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLTK is famous package in python for carry out NLP tasks. Though there are other options also available such as Spacy, TextBlob etc\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit) is a python package speicalized to do NLP tasks. It also comes with a lot of corpuses (a corpus is specialized knowldege for a specific domain/problem. e.g. gender names, location names, or author notes). We need to download these corpus explicitly along with nltk. This can be done through nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we have to do analysis at sentences level. For example in author classificaion problem, we want to have a <b>feature number of words in a sentence</b>. or I want to try out a feature how many times an author uses punctuations in a sentence. or how many capital letters he uses in a sentence. So the list is endless.\n",
    "\n",
    "Though it looks simple to split text by a period(.), but we often use period within text as well e.g. Mr. Modi. So NLTK comes with a trained model to do sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3fe6b063759a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/C50train/AaronPressman/2537newsML.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \"\"\"\n\u001b[0;32m     96\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;31m# Standard word tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m         \"\"\"\n\u001b[1;32m-> 1235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \"\"\"\n\u001b[1;32m-> 1283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \"\"\"\n\u001b[0;32m   1313\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m     \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "text=open(\"./data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "print(\"\\n\".join(sents[1:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization breaks unstructured data, text, into chunks of information which can be counted as discrete elements.\n",
    "These counts of token occurrences in a document can be used directly as a vector representing that document.\n",
    "This immediately turns an unstructured string (text document) into a structured, numerical data structure suitable for machine learning.\n",
    "-  Tokenization segments a document into its atomic elements (tokens). Tokens are generally split by space or punctuations.\n",
    "-  Typically, tokens are the words.\n",
    "-  Sometimes, based over our application, we can treat punctuations as tokens as well. For example, for author identification we want to have a feature, <i>number of punctuations<i>.\n",
    "-  Number of tokens in a sentence can be used as a feature.\n",
    "-  Kind of tokens(words) used in text itself can be a feature. For example, some author may use word basically, additionally etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Justice', 'Department', 'should', \"n't\", 'be', 'singled', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens=word_tokenize(\"The Justice Department shouldn't be singled out.\")\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK package comes with built in trained model for tokenization, that split text into tokens considering various punctuations and spaces. Though its simple to tokenize text using a regular expressions. But its advisable to use NLTK tokenizer which is more intelligent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech tags are grammatical consituents (Noun, Verbs, Adverb, Adjectives) and this process of POS tagging classify tokens into their part-of-speech tags and label them according the tagset which is a collection of tags used for the pos tagging. Part-of-speech tagging also known as word classes or lexical categories. Here is the definition from wikipedia:\n",
    "    \n",
    "<i>In corpus linguistics, part-of-speech tagging (POS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition, as well as its context—i.e. relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill’s tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.<i>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Justice', 'NNP'),\n",
       " ('Department', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('singled', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These DT, NNP, MD etc are pos tags taken from the standard list of Penn TreeBank Tagsets. It can be found here\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "POS tagging is one of the basic and very important component of NLP, as NLP mainly works on linguistics, i.e. way of writing language and Grammar is important part of it. POS tagging in the world of NLP is solved problem and works well if language is written well formatted.\n",
    "\n",
    "POS tagging is also supervised learning solution that uses features like previous word, next word, is first letter capitalized etc.\n",
    "\n",
    "NLTK has a function to get pos tags and it works after tokenization process. \n",
    "\n",
    "In our problem of Author Identification, we can create multiple features using POS Tagging.\n",
    "1. Number of Nouns, Verbs, Adjectives etc.\n",
    "2. How many times sentence starts with Adverb. Meaning words like Basically, Typically etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "nltk.help.upenn_tagset(\"RB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are often treated as useless words those doesnt have any meaning in the text. For example words such as in, to, the, is etc. During NLP, as a part of text preprocess, we tend to remove those stop words. So we only left with meaningful words in the text.\n",
    "\n",
    "Generally there is a standard list of stop words and that list is also included within the NLTK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Justice', 'Department', 'should', \"n't\", 'be', 'singled', 'out', '.']\n",
      "['The', 'Justice', 'Department', \"n't\", 'singled', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Stop-Words are removed before building vectorizer.\n",
    "-  Sometimes we even uses stop words as a feature as well. For example, in author identification, number of stop words can be used as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming And Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are the basic text processing methods for English text. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. \n",
    "\n",
    "-  Consider words like going, gone, go have same meanings if we convert all of these words into base form i.e. go.\n",
    "-  Stemming helps to increase counts of words which have similar meaning.\n",
    "-  Stemming works based over set of rules, such as remove ing if words is ending with ing.\n",
    "-  There are a number of Stemmers available with different set of rules. PorterStemmer, LancasterStemmer, SnowballStemmer etc are example of stemmers and are available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cri'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----PorterStemmer------\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem(\"crying\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----LancasterStemmer------\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem(\"crying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cri'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----SnowballStemmer------\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_stemmer.stem(\"crying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is closely related to Stemming, but difference is lemmatization uses a knowledgebase called WordNet. Because of knowledge, lemmatization can even convert words which are different and cant be solved by stemmers, such as converting went into go. The NLTK Lemmatization method is based on WordNet’s built-in morphy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'go'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"went\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is a process of extracting phrases(aka chunks) from unstructured text. Instead of just simple tokens which may not repersent actual meaning of text, its advisable to use phrases such as \"New Delhi\" as a single word instead of New and Delhi separate words.\n",
    "\n",
    "- Chunking is done using linguistic rules(language grammar rules), such as when two proper nouns occur together, merge them to make a single word. For Example \"South Africa\".\n",
    "-  Chunking works on top of POS tagging, it uses pos-tags as input and provide chunks as output. \n",
    "-  Similar to POS tags, there are standard set of Chunk tags like Noun Phrase(NP), Verb Phrase (VP) etc.\n",
    "-  Most data scientist uses N-Grams instead of chunker, but n-grams ends up creating a lots and lots of meaningless words.\n",
    "-  Chunking is very important when you want to extract information from text such as Locations, Person Names etc. In NLP called Named Entity Extraction.\n",
    "-  In Author Identification, we can hvae features like how many Named entity author uses in a sentence.\n",
    "-  What kind of countries/continents, author mostly refer in his articles.\n",
    "\n",
    "There are a lot of libraries which gives phrases out-of-box such as Spacy or TextBlob. NLTK just provides a mechanism using regular expressions to generate chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Unidentified/JJ)\n",
      "  hackers/NNS\n",
      "  (VP gained/VBD)\n",
      "  (NP access/NN)\n",
      "  to/TO\n",
      "  (NP the/DT department/NN)\n",
      "  's/POS\n",
      "  (NP web/JJ page/NN)\n",
      "  (P on/IN)\n",
      "  August/NNP\n",
      "  16/CD\n",
      "  and/CC\n",
      "  (VP replaced/VBD)\n",
      "  it/PRP\n",
      "  (PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "  (VP labelled/VBD)\n",
      "  (NP the/DT)\n",
      "  Department/NNP\n",
      "  (P of/IN)\n",
      "  Injustice/NNP\n",
      "  that/WDT\n",
      "  (VP included/VBD)\n",
      "  (NP a/DT swastika/NN)\n",
      "  and/CC\n",
      "  (NP a/DT picture/NN)\n",
      "  (P of/IN)\n",
      "  Adolf/NNP\n",
      "  Hitler/NNP\n",
      "  ./.)\n",
      "(NP Unidentified/JJ)\n",
      "(VP gained/VBD)\n",
      "(NP access/NN)\n",
      "(NP the/DT department/NN)\n",
      "(NP web/JJ page/NN)\n",
      "(P on/IN)\n",
      "(VP replaced/VBD)\n",
      "(PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "(P with/IN)\n",
      "(NP a/DT hate-filled/JJ diatribe/NN)\n",
      "(VP labelled/VBD)\n",
      "(NP the/DT)\n",
      "(P of/IN)\n",
      "(VP included/VBD)\n",
      "(NP a/DT swastika/NN)\n",
      "(NP a/DT picture/NN)\n",
      "(P of/IN)\n"
     ]
    }
   ],
   "source": [
    "#Define your grammar using regular expressions\n",
    "#For example a phrase starting with determiners(The/an/a) followed by noun or adjective will be a noun phrase. such as \"a greedy dog\"\n",
    "parser = ('''\n",
    "    NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "    P: {<IN>}           # Preposition\n",
    "    PP: {<P> <NP>}      # PP -> P NP\n",
    "    VP: {<V.*> <PP|RB|V.*>*}  # VP -> V (NP|PP)*\n",
    "    ''')\n",
    "line=\"Unidentified hackers gained access to the department's web page on August 16 and replaced it with a hate-filled diatribe labelled the Department of Injustice that included a swastika and a picture of Adolf Hitler.\"\n",
    "chunkParser = nltk.RegexpParser(parser)\n",
    "negation_result={}\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(line))\n",
    "tree = chunkParser.parse(tagged)\n",
    "negated_entity=\"\"\n",
    "negated_value=\"\"\n",
    "negation=None\n",
    "for subtree in tree.subtrees():\n",
    "    print subtree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Tree Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the in-depth topic in NLP is Lexical Analysis of text wherein we try to analyze and understand text. This process is called deep tree parsing in NLP world where we try to analyze relationships amongst the text.\n",
    "- Text parsing is important when you want to know relationships in text. For example <i>Delhi is capital of India<i>, here Delhi and India are related and having a relationship <b>is capital of<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "sent = \"the dog saw a man in the park\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well we have to define our grammar, which looks quite tedious job. But there are other NLP packages such as Stanford CoreNLP which provides funcitons to generate parse tree from unstructured text without defining any grammar.\n",
    "- Parse tree provides us meaningful and true relations and also kind of relations they share. Also called facts.\n",
    "- Tree Parsing is used to build knowledge base from unstructured corpus. Check DbPedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
